{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwyWBsafruCaq2siRBcIqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiiMceva/LLM_Class_Ziinat/blob/main/3token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gdfgzcv42J",
        "outputId": "0ae12bca-8cea-42fd-c0ec-9c5695bf44c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:  {'<UNK>': 0, 'I': 1, 'love': 2, 'kpop': 3, 'i': 4, 'also': 5, 'bts.': 6}\n",
            "Encoded:  [0, 4, 0, 0]\n",
            "decoded:  ['<UNK>', 'i', '<UNK>', '<UNK>']\n"
          ]
        }
      ],
      "source": [
        "from fontTools.misc.cython import returns\n",
        "import re\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # Vocabulary dictionary\n",
        "        self.word2id = {\"<UNK>\": 0}\n",
        "        self.id2word = {0: \"<UNK>\"}\n",
        "        self.idNext = 1\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        for text in texts:\n",
        "            tokens = text.split()\n",
        "            for token in tokens:\n",
        "                if token not in self.word2id:\n",
        "                    self.word2id[token] = self.idNext\n",
        "                    self.id2word[self.idNext] = token\n",
        "                    self.idNext += 1\n",
        "\n",
        "    def encode(self, texts):\n",
        "        tokens = texts.split()\n",
        "        return [self.word2id.get(token, 0) for token in tokens] # 0 = <UNK>\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return [self.id2word.get(id) for id in ids]\n",
        "\n",
        "# 1. build_vocab(texts)\n",
        "texts = [\"I love kpop\", \"i also love bts.\"]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.build_vocab(texts)\n",
        "print(\"Vocabulary: \", tokenizer.word2id)\n",
        "\n",
        "# 2. Encode(texts)\n",
        "encoded = tokenizer.encode(\"Hello i am Ziint.\")\n",
        "print(\"Encoded: \", encoded)\n",
        "\n",
        "# 3. decode(ids)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(\"decoded: \", decoded)"
      ]
    }
  ]
}